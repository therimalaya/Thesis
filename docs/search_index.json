[
["index.html", "Exploration of Multi-Response Multivariate Methods Introduction", " Exploration of Multi-Response Multivariate Methods Raju Rimal Norwegian University of Life Sciences Faculty of Chemistry, Biotechnology and Food Science raju.rimal@nmbu.no 08, 2019 Introduction Rapid development in technology and computational power have resulted in heaps of data. Extracting information from this chaotic heaps of data has become another problem. Many statistical and machine learning tools are devised for this purpose, most of which focus to identify the relationships between different variables. A linear relationship is the most common assumption. This thesis confined itself in the exploration of linear relationships, where a set of independent variables, called predictor variables, affect another set of dependent variables, called response variables. The space spanned by the columns of predictors and responses are termed predictor space and response space, respectively. Many projection-based statistical methods such as Principal Components Regression (PCR), Partial Least Squares (PLS) Regression and some variants of Envelopes only consider a subspace of predictor space relevant for defining the linear relationship between the predictors and the response(s). This brings us to the concept of relevant and irrelevant space introduced by Naes and Martens (1985). The relevant space can be described as the subspace that contains all the required information to define the relationship between the predictors and the response in a model. The irrelevant space, on the other hand, does not contain any information regarding this relationship. Latent components corresponding to predictor variables, which we will refer to as “predictor components”, are linear combinations of the predictor variables. Naes and Martens (1985) and later Helland (1990), Næs and Helland (1993) and Helland and Almøy (1994) have defined a set of predictor components as irrelevant components if they have no correlation with the response variables and the relevant part. Using only a subset of the latent components for modeling, is often termed as “dimension reduction”. Methods like PCR, PLS and many other variants of PLS has leveraged this concept and are serving as prime tools in many disciplines, most notably in chemometrics. Relatively new methods based on the concept of “envelopes” introduced by Cook, Li, and Chiaromonte (2007), more specifically envelope in predictor variable (Xenv), have also used this concept of dimension reduction. In addition, envelope in response variable (Yenv) and simultaneous envelope in predictor and response (Senv) have extended the concept of relevant and irrelevant space to the response space as well, which they referred to as material and immaterial part. These methods are discussed in Background section. Despite having similar underlying population model, these methods estimate the model parameters differently. Model parameters are the unknowns, which help to define a complex relationships between the variables. Regression coefficient vector \\((\\beta)\\) in (2) is an example of a model parameter. All methods use data to estimate these parameters. So, the properties of a dataset affect the estimation and consequently the prediction performance of the methods. Evaluation of these methods is essential to understand how they interact with various properties of the data. This thesis will explore some of these methods and assess their estimative and predictive strength and weaknesses through both simulated and real datasets. This exploration adds a reference for researchers to motivate them for using different methods based on the properties of the data they are working on. This study is exploratory in nature where we assess and compare different multi-response multivariate methods, but most importantly study their interaction with the properties of the data. The properties include the correlation between predictor variables, the position of principal components of predictor variables (predictor components) that are relevant for certain principal components of the response variables (response components), the amount of correlation between the response variables and the number of predictor variables. The effect of the correlation structure of the response matrix is less explored and it is expected to shed some light on how similar and how different the methods are in terms of modelling this structure. In order to simulate data with these properties varying at different levels, we have created an R-package called simrel, which is an extension of the previous version introduced by Sæbø, Almøy, and Helland (2015) to incorporate multiple responses. "],
["background.html", "Background Multivariate Linear Regression Model Relevant Space and Relevant Components Simulation Estimation and Prediction Multivariate Methods Experimental Design Analysis of Variance", " Background This section discusses the relevant topics that have been used in the included papers. Multivariate Linear Regression Model The joint normal distribution of a random variable-vector \\(\\mathbf{y}\\) of \\(m\\) response variables with mean of \\(\\boldsymbol{\\mu}_y\\) and another random variable-vector \\(\\mathbf{x}\\) of \\(p\\) predictor variables with mean \\(\\boldsymbol{\\mu}_x\\) as, \\[\\begin{equation} \\begin{bmatrix} \\mathbf{y}\\\\ \\mathbf{x} \\end{bmatrix} \\sim \\mathsf{N} \\begin{pmatrix} \\begin{bmatrix} \\boldsymbol{\\mu_y}\\\\ \\boldsymbol{\\mu_x} \\end{bmatrix}, \\begin{bmatrix} \\boldsymbol{\\Sigma_{yy}} &amp; \\boldsymbol{\\Sigma_{yx}} \\\\ \\boldsymbol{\\Sigma_{xy}} &amp; \\boldsymbol{\\Sigma_{xx}} \\end{bmatrix} \\end{pmatrix} \\tag{1} \\end{equation}\\] where \\(\\boldsymbol{\\Sigma}_{xx}\\) and \\(\\boldsymbol{\\Sigma}_{yy}\\) are the variance-covariance matrices of \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\), respectively, and \\(\\boldsymbol{\\Sigma}_{xy} = \\boldsymbol{\\Sigma}_{yx}^t\\) is the covariances between them. A model that linearly relates \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) through regression coefficient vector \\(\\boldsymbol{\\beta}\\) is often written as, \\[\\begin{equation} \\mathbf{y} = \\boldsymbol{\\mu}_y + \\boldsymbol{\\beta}^t\\left(\\mathbf{x} - \\boldsymbol{\\mu}_x\\right) + \\boldsymbol{\\varepsilon} \\tag{2} \\end{equation}\\] where \\(\\boldsymbol{\\varepsilon} \\sim \\textsf{N}\\left(\\mathbf{0}, \\boldsymbol{\\Sigma}_{y|x}\\right)\\) We can write the regression coefficient \\(\\boldsymbol{\\beta} = \\boldsymbol{\\Sigma}_{xx}^{-1}\\boldsymbol{\\Sigma}_{xy}\\) in terms of the covariance matrices. A complete simulation of this model requires to specify \\(1/2(p+m)(p+m+1)\\) unknowns. With a transformation defined as \\(\\mathbf{z} = \\mathbf{Rx}\\) and \\(\\mathbf{w} = \\mathbf{Qy}\\) with \\(\\mathbf{R}_{p\\times p}\\) and \\(\\mathbf{Q}_{m\\times m}\\) as random orthogonal rotation matrices, model (1) can be rewritten as, \\[\\begin{align} \\begin{bmatrix}\\mathbf{w} \\\\ \\mathbf{z}\\end{bmatrix} &amp; \\sim N \\left(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}\\right) = N \\left( \\begin{bmatrix} \\boldsymbol{\\mu}_w \\\\ \\boldsymbol{\\mu}_z \\end{bmatrix}, \\begin{bmatrix} \\boldsymbol{\\Sigma}_{ww} &amp; \\boldsymbol{\\Sigma}_{wz} \\\\ \\boldsymbol{\\Sigma}_{zw} &amp; \\boldsymbol{\\Sigma}_{zz} \\end{bmatrix} \\right) \\nonumber \\\\ &amp;= N \\left( \\begin{bmatrix} \\boldsymbol{Q\\mu}_y \\\\ \\boldsymbol{R\\mu}_x \\end{bmatrix}, \\begin{bmatrix} \\boldsymbol{Q\\Sigma}_{yy}\\boldsymbol{Q}^t &amp; \\boldsymbol{Q\\Sigma}_{yx}\\mathbf{R}^t \\\\ \\boldsymbol{R\\Sigma}_{xy}\\boldsymbol{Q}^t &amp; \\boldsymbol{R\\Sigma}_{xx}\\mathbf{R}^t \\end{bmatrix} \\right) \\tag{3} \\end{align}\\] Since both \\(\\mathbf{Q}\\) and \\(\\mathbf{R}\\) are orthonormal matrices, i.e., \\(\\mathbf{Q}^t\\mathbf{Q} = \\mathbf{I}_m\\) and \\(\\mathbf{R}^t\\mathbf{R} = \\mathbf{I}_p\\), the inverse transformation can be defined as, \\[\\begin{equation} \\begin{matrix} \\boldsymbol{\\Sigma}_{yy} = \\mathbf{Q}^t \\boldsymbol{\\Sigma}_{ww} \\mathbf{Q} &amp; \\boldsymbol{\\Sigma}_{yx} = \\mathbf{Q}^t \\boldsymbol{\\Sigma}_{wz} \\mathbf{R} \\\\ \\boldsymbol{\\Sigma}_{xy} = \\mathbf{R}^t \\boldsymbol{\\Sigma}_{zw} \\mathbf{Q} &amp; \\boldsymbol{\\Sigma}_{xx} = \\mathbf{R}^t \\boldsymbol{\\Sigma}_{zz} \\mathbf{R} \\end{matrix} \\tag{4} \\end{equation}\\] Here, \\(\\boldsymbol{\\Sigma_{zz}}\\) and \\(\\boldsymbol{\\Sigma_{ww}}\\) are diagonal matrices of eigenvalues corresponding to predictors and responses respectively. Following the concept of relevant components \\(\\boldsymbol{\\Sigma}_{wz}=\\boldsymbol{\\Sigma}_{zw}^t\\) has non-zero elements for relevant components. With some random orthogonal rotation matrices \\(\\mathbf{R}\\) and \\(\\mathbf{Q}\\), which can be easily generated, the unknowns required for simulation may drastically decrease. Following the idea from Sæbø, Almøy, and Helland (2015), Paper I uses exponential decay of eigenvalues, as in (5), that fills the diagonals of \\(\\boldsymbol{\\Sigma}_{zz}\\) and \\(\\boldsymbol{\\Sigma}_{ww}\\). Here the decay factor \\(\\gamma\\) controls the multicollinearity such that a higher value of gamma corresponds to high multicollinearity. \\[\\begin{equation} \\lambda_i = e^{-\\gamma(i - 1)}, \\gamma &gt;0 \\text{ and } i = 1, 2, \\ldots, p \\tag{5} \\end{equation}\\] A thorough discussion on the reparameterization of a linear model to simulate data by the concept of “relevant components” can be found in Paper I. The following subsection discusses the concept of relevant components in brief. Relevant Space and Relevant Components In the model (1), not all information in \\(\\mathbf{x}\\) is relevant for \\(\\mathbf{y}\\) and not all variation in \\(\\mathbf{y}\\) is explainable or non-redundant. We can refer to the space “with information” as relevant (informative) space and the rest as irrelevant (uninformative) space. Naes and Martens (1985) introduced the definition of relevant space as the decomposition of the predictor space into two orthogonal subspaces: the relevant and the irrelevant space. Additionally, a set of predictor components defined as irrelevant components do not have any correlation with the response and the relevant part of the data. The relevant components, on the other hand, contains all the required information to explain the variation in the response \\(\\mathbf{y}\\). Multivariate methods such as Principal Components Regression (PCR) and Partial Least Squares (PLS) Regression uses the eigenvectors to span the relevant and irrelevant spaces. Here, we refer the eigenvectors that span the relevant space as relevant eigenvectors. The concept was further discussed and developed by Helland (1990), Næs and Helland (1993) and Helland and Almøy (1994). However, all these studies have discussed the separation of relevant and irrelevant space only in the predictor space. More recently, various estimators (Cook, Li, and Chiaromonte 2010; R. D. Cook, Helland, and Su 2013; R. Dennis Cook and Zhang 2015b) based on a so-called “envelope” (Cook, Li, and Chiaromonte 2007) have used and extended the concepts of the separation of relevant and irrelevant spaces to the response space as well. The relevant and irrelevant spaces are referred to as material and immaterial spaces in their literature (Figure 1). The envelope methods use “envelope”, a linear combination of relevant eigenvectors (Cook 2018), to span the relevant space. Figure 1: A heuristic illustration of relevant and irrelevant spaces in a response space and a predictor space To elaborate on the concept of relevant components and how they interact with other properties and influence the prediction of methods, let us consider an example. Assume a single response model with 10 predictor variables where the information contained in these 10 predictors can be completely explained by four principal components of \\(\\Sigma_{xx}\\), the variance-covariance matrix of the predictor (\\(\\mathbf{x}\\)). These four components are the relevant components. Consider two cases: Case 1 (Figure 2, left): The position of these relevant components are 1, 2, 3 and 4. The eigenvalues of \\(\\Sigma_{xx}\\) decay slowly, i.e. low multicollinearity. Here, the relevant components from 1 to 4 have large variation, so that, most methods easily extract the information and fit the model quite accurately. Case 2 (Figure 2, right): The position of the relevant components are at 5, 6, 7 and 8. The eigenvalues of \\(\\Sigma_{xx}\\) decay rapidly, i.e. high multicollinearity. Here the relevant components from 5 to 8 have small variation, so that, it is difficult for most methods to extract the information and fit the model. Figure 2: Relevant components at two different set of positions and two different levels of multicollinearity. The points represents the correlation of predictor components and the response variable. The grey bars are the eigenvalues of \\(\\boldsymbol{\\Sigma}_{xx}\\). Further, PCR and PLS regression are used with the data simulated from these two cases. Also, leave-one-out cross-validation validates their prediction performance, and the root mean squares error of prediction measures their prediction error (Figure 3). Different methods target these cases differently. For example, PCR tries to capture maximum variation in \\(\\mathbf{x}\\) through principal components, so it starts reducing its prediction error only after including the relevant components. For this method, in the first case, prediction error starts decreasing from the first component on, and stabilize after the fourth component while in the second case, prediction error only starts decreasing after the fifth component. This method requires all four relevant components to get the minimum prediction error. Partial Least Square Regression (PLS), on the other hand, is motivated to maximize the covariance between the predictors and the response. We can see a significant decline of prediction error after the first relevant components is included but it uses fewer components to get the minimum prediction error than PCR in both cases. Helland and Almøy (1994) has shown a similar result and shown that the relevant components with small variation make the prediction difficult. Figure 3: Root mean square error of cross-validation from PCR and PLSR The concept of relevant components can also be extended to the response such that a subspace contains the information relevant for a model. The concept is implemented in the simultaneous envelope (R. Dennis Cook and Zhang 2015b) and the response envelope (Cook, Li, and Chiaromonte 2010) methods. Simulation Random variables are the basic components of a complex model and a stochastic simulation. These random variables can be generated on a computer by sampling and manipulating uniform random variables \\(U(0, 1)\\) which requires random numbers. Although computers can not generate truly random numbers, it can, however, generate pseudo-random numbers. These numbers appear as random numbers but they are completely deterministic. Since they are deterministic, any experiment performed using these numbers can be repeated exactly (Jones, Maillardet, and Robinson 2014). We can use these uniform random variables to create other random variables that follow a certain distribution. Standard Normal Distribution is a common one and is used in many statistical simulations including the tool discussed in paper I. Given that we can simulate a standard normal variable \\(\\mathbf{z}\\), one can obtain any normal distribution with arbitrary mean \\(\\mu\\) and variance \\(\\sigma^2\\) as \\(\\mu + \\sigma Z\\). Here, we can control the parameters \\(\\mu\\) and \\(\\sigma\\). Simulation refers to generating data from a known underlying population structure. Controlling the properties of the population is vital in the simulation. This enables researchers and users to use data for comparison of methods, assessing new methodology, testing theory and evaluating algorithms. Such data can also be used for educational purposes. All the research studies in this thesis have used an R-package called simrel for simulating multi-response linear model data (paper I). The simulation tool is general purpose in nature and has a limited number of parameters that controls the essential properties of the population. It is flexible and enables users to simulate data with a wide range of properties. Some of these properties include the level of correlation between the predictors (gamma) and responses (eta) through exponential decay factor as in (5). The position of the relevant components (relpos), the number of predictor variables (p) and the number of response variables (m) can also be controlled during the simulation. Estimation and Prediction Measures such as mean and standard deviation for a population are usually referred to as parameters of the population. A model as in (2), which expresses the relationship between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) in the population, uses parameters such as the error variance and regression coefficients. Usually, due to the lack of known population distribution, the values of these parameters are calculated using a sample collected from the population. The process of determining the value of certain parameters is called estimation. The estimated parameter values from any two samples are different. A method for estimation is considered better if the expected squared difference between the estimated and true value is small and has small variance. The goodness of estimation method depends on the nature of the data. Estimation error with true and estimated regression coefficient \\(\\boldsymbol{\\beta}\\) and \\(\\boldsymbol{\\widehat{\\beta}}\\) respectively, can be defined as in (6). \\[\\begin{equation} \\text{Estimation Error} = \\mathsf{E}\\left[ \\left(\\boldsymbol{\\beta} - \\hat{\\boldsymbol{\\beta}}\\right)^t \\left(\\boldsymbol{\\beta} - \\hat{\\boldsymbol{\\beta}}\\right) \\right] \\tag{6} \\end{equation}\\] A fitted or trained model is mostly used for prediction. Prediction refers to determining the value of the response for a new set of predictors, which were not used to train the model. Most studies under “data science” field are targeted for better prediction. Most comparisons in this thesis evaluate the prediction performance of the multivariate methods using the prediction error measured as in (7). \\[\\begin{equation} \\text{Prediction Error} = \\mathsf{E}\\left[ \\left(\\boldsymbol{\\beta} - \\hat{\\boldsymbol{\\beta}}\\right)^t \\boldsymbol{\\Sigma}_{xx} \\left(\\boldsymbol{\\beta} - \\hat{\\boldsymbol{\\beta}}\\right) \\right] + \\boldsymbol{\\Sigma}_{y|x} \\tag{7} \\end{equation}\\] From (6) and (7), we can see that the prediction errors are influenced by the covariance of the predictors directly, while estimation error is not. In the case of multicollinear predictors, estimation error can be huge, while due to the scaling of the covariation of predictors, the prediction error can still be small. A good estimation can give a proper and trustworthy idea about the relation between certain predictor variation with a certain response variable. This is important in policymaking, academic researches and to understand the relationships when developing new models. Prediction, on the other hand, is widely used from weather forecasting, economic forecasting, prediction in production and sales, and many more. Multivariate Methods Various multivariate methods such as ordinary least squares (OLS), principal components regression (PCR), partial least squares (PLS) regression and envelope methods are used for comparative studies included in this thesis. All of these methods except OLS use the concept of relevant space and the reduction of the regression model. Here we will refer PLS2, which models all the response variables together, as PLS and PLS1, which models each responses separately, as PLS1. Methods based on Envelope Model Three different methods based on envelopes are also included for comparison. Cook, Li, and Chiaromonte (2007) defined envelope as the smallest subspace that includes the span of true regression coefficients and developed various estimators based on the concept of the envelope through various subsequent papers. Response envelope (Yenv) (Cook, Li, and Chiaromonte 2010) performs dimension reduction only in the response space while Predictor envelope (Xenv) (R. D. Cook, Helland, and Su 2013) performs dimension reduction only in the predictor space. The simultaneous envelope (Senv) (R. Dennis Cook and Zhang 2015b) performs dimension reduction on both predictor and response space simultaneously. If all the possible components (latent dimension) are included in these methods, the results are equivalent to OLS regression. The comparisons of these envelope methods together with PCR and PLS in the third and fourth paper have shown encouraging results for envelope methods in both easy and difficult cases. PLS and its derivatives Since the PLS method has been both popular and productive in fields like chemometrics, its development has progressed quickly over time through the formulation of various derivatives. CPLS and CPPLS are among them which combines PLS and canonical correlation analysis (CCA) and give a joint framework for classification and regression (Indahl, Liland, and Næs 2009). Paper-I has made some basic comparison of these methods for their predictive ability. More recently, Helland, Saebø, and Tjelmeland (2012)] introduced the Bayes PLS method. The method only works with a single response model and has shown promising results compared to other methods in Paper-II. Wentzell and Montoto (2003) has assembled many comparisons made on PCR and PLS where they conclude that PLS has not shown a clear advantage over PCR over predictive ability in most studies, but uses fewer components than PCR. Many studies are available comparing PCR, PLS and their derivatives. However, there are not any studies to date which have made any empirical comparisons of the newly developed envelope based methods using real and simulated data with these more established methods. Details on each of these methods can be obtained from the corresponding references. Experimental Design In all the post hoc comparisons, simulation parameters are considered as independent variables (factors), and the prediction- and estimation errors are considered as outcome variables (responses). Factorial Design is implemented as an experimental design which allowed us to compare all possible combination of different factor levels. For example, the factorial design used throughout the third and fourth paper, shown in Figure 4, has four factors: a) Number of predictor variables (p) with two levels, b) level of multicollinearity (gamma) with two levels, where higher value represents a higher level of multicollinearity, c) position index of relevant predictor components (relpos) and d) the level of collinearity in response (eta), with four levels where higher value represents a higher correlation between the response variables. The combination of these factors has created 32 unique designs which are then used for simulating data with those particular properties. Such data, with all possible combination of these properties, have made both thorough and rigorous comparison possible. Figure 4: An example of a factorial design used in the third and fourth paper. Let us dig a little deeper to understand how these simulation parameters are tied with the properties of the simulated data. As an example, let us take Design 1 and Design 9 of Figure 4 where data simulated with Design 1 have low multicollinearity and the position index of relevant components are 1, 2, 3, 4, while Design 9 have high multicollinearity and the position index of relevant components are 5, 6, 7, 8. With other factors or properties of the data being the same for both, the difference in these two designs help us to analyse the interaction between the multicollinearity in the data and the position of relevant components on, for instance, prediction performance of the methods. Figure 5: Design 1: Relevant components have large variation, Design 9: irrelevant components have large variation and relevant components have small variation. Figure 5 (top-row) shows the scaled covariance between the predictor components and the response variables for Design 1. Here the relevant components with larger variation (due to low multicollinearity) simulate data that are easier to model by most methods. Figure 5 (bottom-row) for Design 9 shows that the relevant components at position 5, 6, 7, 8 have small variation and irrelevant components at position 1, 2, 3, 4 have large variation. This design simulates data that are difficult to model by most methods. The population covariances in the figure give clear and distinct relationship, while the sample covariances give a somewhat rough approximation of the population. Analysis of Variance The analysis in these studies have used various exploratory plots of prediction error, estimation error and the number of components used by different methods. Also, visualizations from principal components analysis (PCA) have been used on these errors. Besides, a more formal analysis is made using analysis of variance (ANOVA). ANOVA allowed us not only to understand the effect of various properties of data controlled by the simulation parameters but also analyses the effect of the interaction of these properties with the methods. The third and fourth paper use multivariate analysis of variance (MANOVA) to analyze the effect on four response variables. MANOVA is the multivariate counterpart of the ANOVA where various test statistic are used, such as Wilks’ Lambda, Lawley-Hotelling trace, Pillai trace and Roy’s largest root. All of these methods use the within \\((\\mathbf{E})\\) and between \\((\\mathbf{H})\\) sum of squares and the cross products matrices. All four test statistic are nearly equivalent for large sample size (Johnson and Wichern 2018). In our studies, Pillai trace is used, which is defined as, \\[\\begin{equation} \\text{Pillai statistic} = \\text{tr}\\left[(E + H)^{-1}H\\right] = \\sum_{i=1}^{m}{\\frac{\\nu_i}{1+\\nu_i}} \\tag{8} \\end{equation}\\] where, \\(\\nu_i\\) represents the eigenvalues corresponding to \\(\\mathbf{E^{-1}H}\\). "],
["paper-summary.html", "Paper Summary Paper 1: A Tool for Simulating Multi Response Linear Model Data Paper 2: Model and Estimators for PLS Regression Paper 3: Comparison of Multi Response Prediction Methods Paper 4: Comparison of Multi Response Estimation Methods", " Paper Summary Paper 1: A Tool for Simulating Multi Response Linear Model Data As an extension of Sæbø, Almøy, and Helland (2015) to simulate linear model data with multiple response variables, this paper discusses the simulation model, the strategy for simulation, and compares some multivariate methods using simulated data. Additionally, it includes an R-package called simrel that is built based on the mathematical formulation discussed in the paper. The simulation of the linear model discussed here is based on the concept of the relevant components. A subspace of the predictor space, which is relevant for a subspace of response space, is the basis of the simulation tool. These subspaces are assumed to be spanned by a subset of respective latent components. The simulation strategy started with identifying the covariance between these components that satisfy the user’s condition for the data, i.e. the simulation parameters. A covariance structure of the latent space is then created which is rotated by an arbitrary orthogonal rotation matrix to obtain the population covariance structure of the simulated data. Data is then sampled from a normal distribution with the constructed covariance structure. The tool also provides mathematically computed properties of the data such as true regression coefficient, minimum model error, coefficient of determination and the predictor variables relevant for a given response. In addition to the mathematical formulation for simulation, the study compares some multivariate methods including OLS, PCR, PLS and Envelope using two simulation examples. It has included some derivatives of PLS such as PLS1, PLS2, CPLS and CPPLS and some methods based on envelope estimation such as Xenv, Yenv and Senv. The first example has three relevant response components rotated into five response variables. Additionally, four simulation designs were constructed using factorial design with low and high multicollinearity interacting with low and high noise levels. The simultaneous envelope (Senv) method has achieved the smallest prediction error with a smaller number of components in the dataset with low noise level (high coefficient of determination), while canonical PLS (CPLS and CPPLS) have shown better performance in the dataset with a higher level of noise. All the methods are found robust for the multicollinearity problem. The second example compares PLS1 and PLS2 where, on most occasions, the latter dominates the earlier with regard to minimum prediction error. Further, the paper has also introduced the shiny (Chang et al. 2018) web application designed for easier access to the simulation tool. Paper 2: Model and Estimators for PLS Regression Comparison of methods requires us to understand the modelling approach of the corresponding methods. This paper formulates five different ways to present a PLS model (Helland 1990) and shows how they are equivalent. Additionally, it argues that the concept of relevant components for reduction of the regression model is the simplest way for it. My contribution to the paper was to compare the performance of PCR, PLS, Bayes PLS and Envelope (Xenv) methods using both simulated and real data. The comparison was based on simulated data with 32 unique properties through a factorial design of simulation parameters. The parameters include medium and high levels of coefficient of determination, medium and high levels of multicollinearity, four different position index of relevant predictor components and two different \\(n/p\\) ratios, 0.3 and 0.8. The study is based on a single response model. The study found some interesting results for the envelope and Bayes PLS methods. Since the Envelope (Xenv) method is based on maximum likelihood, the designs with \\(n/p\\) ratio equals to 0.8 destroyed its prediction while the method has fine prediction when the ratio was 0.3. Bayes PLS has shown remarkable prediction performance in most design, however, both methods had convergence problem in many situations. Despite having the best performance, Bayes PLS has time-consuming computation and failed to converge for some cases. For practical purpose, the study recommends the ordinary PLS algorithm as a good option for prediction purpose. Paper 3: Comparison of Multi Response Prediction Methods Since prediction has been an essential component in data science, understanding how the prediction methods interact with different properties of data is important. This paper, together with the next, makes a comprehensive comparison using simulated data with specifically designed properties through various simulation parameters. The experimental design in Figure 4, discussed in the previous section, has been used in both of these comparisons. Besides, for the prediction comparison, two real data examples have also been used in the study. These two papers try to give an understanding of the interaction between methods and data properties in multi-response cases and also assess the performance of the envelope methods (Xenv and Senv) using both simulation and data from the field of chemometrics. Further, these studies not only use prediction and estimation error for assessment but also the number of components used to get the minimum error. Here only methods based on relevant space such as PCR, PLS (PLS1 and PLS2) and Envelopes (Xenv and Senv) are considered for comparison. Since envelope methods are unable to fit a model with \\(p&gt;n\\), principal components of the predictor matrix were used to reduce the number of predictors. The number of components that explains the minimum of 97.5% of the variation in \\(\\mathbf{x}\\) are chosen. The regression coefficients were later transformed back using the respective eigenvectors. Since the envelope methods reduce the dimension as part of its fitting process, this detour in \\(p&gt;n\\) cases does not give them extra benefit which we have tested for \\(n&gt;p\\) cases using with and without principal components. This paper also illustrates the use of principal components for implementing envelope methods in data with wide \\((p&gt;n)\\) predictor matrix which is common in fields like chemometrics and bioinformatics. The minimum prediction error and the number of components to get that error are considered as observed responses in the study. The simulation parameters used in the experimental design are considered as factor variables for further statistical analysis. Multivariate Analysis of Variance (MANOVA) is used for proper statistical analysis with third-order interaction of these factors. The effect of different levels of the factors and their interactions are used for minute comparison. Envelope methods in the study have produced a small prediction error using fewer components than other methods. The effect of correlation between the response variables is small for all methods, however, envelope methods are more sensitive to this correlation. All methods are robust for handling multicollinearity, but PCR and PLS methods struggle more when the relevant predictor components have small variance and irrelevant components have a large variance. Example with real data shows PCR and PLS have the smallest prediction error, but the number of components used by these is higher than the envelope methods. Envelope methods in these examples have obtained prediction error closer to the minimum obtained by PCR and PLS, but using a smaller number of components. Paper 4: Comparison of Multi Response Estimation Methods In many disciplines, the correct and stable estimation is just as an important primary objective as the prediction. This paper extends the analysis from Paper 3 to analyze the estimation aspect of the methods. The same experimental design and simulated data are used for this assessment as well. The study found that overall performance highly depends on the nature of the data since simulation parameters, such as multicollinearity level and position of relevant predictors significantly interact with the methods. Since both envelope methods have smaller prediction and estimation error and have used fewer number of components, low multicollinearity with independent response variables are in favour of these methods. Higher correlations between the responses have given a larger estimation error for envelope methods. For these methods, choosing the wrong number of components can result in large estimation error, so the study also suggests using validation for estimation purpose. Both prediction and estimation error from PCR are more stable than other methods, while as PLS1 method models each response separately, the performance in general is poorer than other methods. "],
["discussions-conclusions.html", "Discussions &amp; Conclusions", " Discussions &amp; Conclusions Simulated data are used in many scientific studies and teaching purposes. Assessing the properties of methods or algorithms is essential and usual in the scientific community. Since scientists often spend a lot of time developing a simulation model, paper-I attempts to present a simple, versatile and general-purpose tool for simulating such data only using few parameters. This attempt of adding a tool in scientists’ toolbox aims at making the laborious work of researchers simpler and less time-consuming. Although not discussed much in the paper, the tool can also be useful for teaching purposes. Using the tool, educators can easily simulate data based on their context and need. Most of our comparisons are on the methods that are based on the concept of relevant spaces. The study in paper-II helped us to understand the similarities and differences between these methods. My contribution to the second part of the paper was to use the simulation tool discussed in paper-I to compare these methods empirically. The Bayes PLS method has shown the best performance in these simulation results, and its performance on real data was satisfactory. This pointed us to explore the methods comprehensively. However, due to the time-consuming computation and as the Bayes PLS method has not yet been developed to work with multiple responses, we planned to use only the envelope methods, PCR and PLS for further exploration. The further exploration continued on the multi-response setting for evaluating and comparing PCR, PLS and two envelope methods (Xenv and Senv) for their performance on prediction and estimation. These methods are capable of modelling multi-response models and are based on the concept of relevant space and dimension reduction. Prediction and estimation both have many aspects to be discussed, we have divided the comparison study into two papers: Paper-III and Paper-IV. Since both papers use the same simulated data based on the same experimental design, it became easier to make comparisons of prediction and estimation for individual methods. Since multicollinearity highly interacts with the position of the principal components, these factors highly influence both estimation and prediction. These factors were used as simulation parameters in addition to a factor that controls the correlation between the response variables. The study on the response correlation and its interaction with these methods and other simulation parameters are limited. This studies’ attempts to fill up the gap have made this thesis novel and useful. In the last two papers, Envelope methods have shown fine performance, specifically in the simulation examples. The PCR method has shown good performance if an optimal number of component is used. The performance is also stable, even with non-optimal number of components. Both PLS1 and PLS2 have stable and better performance, particularly when relevant components are at the initial position (i.e. with large variation). The fine performance of envelope methods is achieved using a smaller number of components, which shows its remarkable strength in dimension reduction. An optimal number of components is crucial for the Envelope methods than for the PCR and PLS methods, as the estimation error rapidly increases with an increasing number of non-optimal components. In general, the study encourages researchers for using newly developed methods such as the envelope. This kind of comparisons in chemometrics data is relatively new for both chemometrics fields and the envelope methods. This thesis also hopes to be a useful reference for other researchers. Since Envelope methods have dimension reduction in response, it can be useful when many responses can be explained by fewer response components. Not a single method is superior for all kinds of data, and using methods correctly requires identifying the properties of data. More sophisticated assessment and comparison can be possible through the tool simrel. Researchers are encouraged to leverage the tool for their study and experiments. We would like to request the developer of the envelope to reach different fields and spread the envelope in a more simple and less mathematical form of communication. "],
["limitations-future-perspectives.html", "Limitations &amp; Future Perspectives", " Limitations &amp; Future Perspectives Although the studies in the thesis are all comparisons of methods, it is important to make those comparisons to evaluate the methods and to understand their interaction with various properties that can exist in real data. This provides an example assessment for method developers and gives a clear understanding of the methods under comparisons for these specific cases to other researchers. The study mostly covers the comparisons through simulated data and some real data, but it also provides a direction for further exploration of these methods and other methods. Ridge, Lasso and other methods could have been used for comparison, but since they are not explicitly based on the concept of relevant components, we have discarded them from these comparisons at this point. Although we did some basic comparison by including them, they require a separate and a more comprehensive study. These studies are highly based on simulated data and somewhat on real data, it could also have been extended to the comparison of their mathematical formulation. This has been done, to some extent, in the second paper for a single response case but the simultaneous envelope and multi-response case need a separate study. In the current state, the simulation tool assumes that the predictor components relevant for one response component are not relevant for others. This can be further studied and can be extended to simulate a more general data structure. Additionally, due to the rise in the popularity of machine learning methods, a similar comparative study of statistical and machine learning methods is also recommended as a future perspective of this study. "],
["tools-and-resources.html", "Tools and Resources", " Tools and Resources R-package: https://github.com/simulatr/simrel Shiny Application: https://github.com/simulatr/AppSimulatr Thesis GitHub Repository: https://github.com/therimalaya/Thesis Paper 1: https://github.com/therimalaya/simrel-m Paper 2: https://github.com/therimalaya/model-comparison-paper Paper 3: https://github.com/therimalaya/03-prediction-comparison Paper 4: https://github.com/therimalaya/04-estimation-comparison "],
["references.html", "References", " References Aldrin, Magne. 2000. “Multivariate prediction using softly shrunk reduced-rank regression.” American Statistician 54 (1): 29–34. https://doi.org/10.1080/00031305.2000.10474504. Almøy, Trygve. 1996. “A simulation study on comparison of prediction methods when only a few components are relevant.” Computational Statistics &amp; Data Analysis 21 (1): 87–107. https://doi.org/10.1016/0167-9473(95)00006-2. Anderson, T. W., I. Olkin, and L. G. Underhill. 1987. “Generation of Random Orthogonal Matrices.” SIAM Journal on Scientific and Statistical Computing 8 (2): 625–29. https://doi.org/10.1137/0908055. Chang, Winston, Joe Cheng, JJ Allaire, Yihui Xie, and Jonathan McPherson. 2018. Shiny: Web Application Framework for R. https://CRAN.R-project.org/package=shiny. Cook, R. Dennis. 2018. An introduction to envelopes : dimension reduction for efficient estimation in multivariate statistics. 1st ed. Hoboken, NJ : John Wiley &amp; Sons, 2018. Cook, R. Dennis, Liliana Forzani, and Zhihua Su. 2016. “A note on fast envelope estimation.” Journal of Multivariate Analysis 150: 42–54. https://doi.org/10.1016/j.jmva.2016.05.006. Cook, R. Dennis, Bing Li, and Francesca Chiaromonte. 2007. “Dimension reduction in regression without matrix inversion.” Biometrika 94 (3): 569–84. https://doi.org/10.1093/biomet/asm038. Cook, R Dennis, Bing Li, and Francesca Chiaromonte. 2010. “Envelope Models for Parsimonious and Efficient Multivariate Linear Regression.” Statistica Sinica 20 (3): 927–1010. Cook, R. Dennis, and Zhihua Su. 2013. “Scaled envelopes: Scale-invariant and efficient estimation in multivariate linear regression.” Biometrika 100 (4): 939–54. https://doi.org/10.1093/biomet/ast026. Cook, R. Dennis, Zhihua Su, and Yi Yang. 2015. “envlp: A MATLAB Toolbox for Computing Envelope Estimators in Multivariate Analysis.” Journal of Statistical Software 62 (8): ??–?? https://doi.org/10.18637/jss.v062.i08. Cook, R. Dennis, and Xin Zhang. 2015a. “Foundations for Envelope Models and Methods.” Journal of the American Statistical Association 110 (510): 599–611. https://doi.org/10.1080/01621459.2014.983235. ———. 2015b. “Simultaneous envelopes for multivariate linear regression.” Technometrics 57 (1): 11–25. https://doi.org/10.1080/00401706.2013.872700. ———. 2016. “Algorithms for Envelope Estimation.” Journal of Computational and Graphical Statistics 25 (1): 284–300. https://doi.org/10.1080/10618600.2015.1029577. Cook, R. D., I. S. Helland, and Z. Su. 2013. “Envelopes and partial least squares regression.” Journal of the Royal Statistical Society. Series B: Statistical Methodology 75 (5): 851–77. https://doi.org/10.1111/rssb.12018. Gamerman, D, and H F Lopes. 2006. Markov Chain Monte Carlo: Stochastic Simulation for Bayesian Inference, Second Edition. Vol. 1. Taylor &amp; Francis. Gangsei, Lars Erik, Trygve Almøy, and Solve Sæbø. 2016. “Theoretical evaluation of prediction error in linear regression with a bivariate response variable containing missing data.” Communications in Statistics - Theory and Methods 0926 (just-accepted): 1–9. https://doi.org/10.1080/03610926.2016.1222434. Gangsei L. E., Almøy T., and Sæbø S. 2016. “Linear regression with bivariate response variable containing missing data. An empirical Bayes strategy to increase prediction precision.” Communications in Statistics – Simulation and Computation. Golub, Gene H, Charles F Van Loan, and C F V Loan. 2012. Matrix computations. Vol. 3. 8. JHU Press. https://doi.org/10.1063/1.3060478. Heiberger, Richard M. 1978. “Algorithm AS 127: Generation of Random Orthogonal Matrices.” Applied Statistics 27 (2): 199. https://doi.org/10.2307/2346957. Helland, Inge S. 1990. “Partial least squares regression and statistical models.” Scandinavian Journal of Statistics 17 (2): 97–114. https://doi.org/10.2307/4616159. ———. 2000. “Model Reduction for Prediction in Regression Models.” Scandinavian Journal of Statistics 27 (1): 1–20. https://doi.org/10.1111/1467-9469.00174. Helland, Inge S., and Trygve Almøy. 1994. “Comparison of prediction methods when only a few components are relevant.” Journal of the American Statistical Association 89 (426): 583–91. https://doi.org/10.1080/01621459.1994.10476783. Helland, Inge S., Solve Saebø, and Ha Kon Tjelmeland. 2012. “Near Optimal Prediction from Relevant Components.” Scandinavian Journal of Statistics 39 (4): 695–713. https://doi.org/10.1111/j.1467-9469.2011.00770.x. Helland, Inge Svein, Solve Saebø, Trygve Almøy, Raju Rimal, Solve Sæbø, Trygve Almøy, and Raju Rimal. 2018. “Model and estimators for partial least squares regression.” Journal of Chemometrics 32 (9): e3044. https://doi.org/10.1002/cem.3044. Indahl, Ulf. 2005. “A twist to partial least squares regression.” Journal of Chemometrics 19 (1): 32–44. https://doi.org/10.1002/cem.904. Indahl, Ulf G., Kristian Hovde Liland, and Tormod Næs. 2009. “Canonical partial least squares-a unified PLS approach to classification and regression problems.” Journal of Chemometrics 23 (9): 495–504. https://doi.org/10.1002/cem.1243. Johnson, R. A., and D. W. Wichern. 2018. Applied Multivariate Statistical Analysis (Classic Version). Pearson Modern Classics for Advanced Statistics Series. Pearson Education Canada. https://books.google.no/books?id=QBqlswEACAAJ. Jolliffe, I T. 2002. Principal Component Analysis, Second Edition. https://doi.org/10.2307/1270093. Jones, Owen, Robert Maillardet, and Andrew Robinson. 2014. Introduction to Scientific Programming and Simulation Using R. Chapman; Hall/CRC. Jong, Sijmen de. 1993. “SIMPLS: An alternative approach to partial least squares regression.” Chemometrics and Intelligent Laboratory Systems 18 (3): 251–63. https://doi.org/10.1016/0169-7439(93)85002-X. Kiers, Henk A. L., and Age K. Smilde. 2007. “A comparison of various methods for multivariate regression with highly collinear variables.” Statistical Methods and Applications. https://doi.org/10.1007/s10260-006-0025-5. Langsrud, Øyvind. 2005. “Rotation tests.” Statistics and Computing 15 (1): 53–60. https://doi.org/10.1007/s11222-005-4789-5. Lee, Minji, and Zhihua Su. 2018. Renvlp: Computing Envelope Estimators. https://CRAN.R-project.org/package=Renvlp. Mevik, Bjørn-Helge, Ron Wehrens, and Kristian Hovde Liland. 2018. Pls: Partial Least Squares and Principal Component Regression. https://CRAN.R-project.org/package=pls. Naes, Tormod, and Harald Martens. 1985. “Comparison of prediction methods for multicollinear data.” Communications in Statistics - Simulation and Computation 14 (3): 545–76. https://doi.org/10.1080/03610918508812458. Næs, Tormod, and Inge S Helland. 1993. “Relevant components in regression.” Scandinavian Journal of Statistics 20 (3): 239–50. Næs, Tormod, Oliver Tomic, Nils Kristian Afseth, Vegard Segtnan, and Ingrid Måge. 2013. “Multi-Block Regression Based on Combinations of Orthogonalisation, Pls-Regression and Canonical Correlation Analysis.” Chemometrics and Intelligent Laboratory Systems 124: 32–42. R Core Team. 2018. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/. Rencher, Alvin C. 2003. Methods of Multivariate Analysis. Vol. 492. John Wiley &amp; Sons. Rimal, Raju, Trygve Almøy, and Solve Sæbø. 2018a. “A Tool for Simulating Multi-Response Linear Model Data.” Chemometrics and Intelligent Laboratory Systems 176: 1–10. https://doi.org/https://doi.org/10.1016/j.chemolab.2018.02.009. ———. 2018b. “A tool for simulating multi-response linear model data.” Chemometrics and Intelligent Laboratory Systems 176 (May): 1–10. https://doi.org/10.1016/j.chemolab.2018.02.009. Rimal, Raju, Trygve Almøy, and Solve Sæbø. 2019. “Comparison of Multi-Response Prediction Methods.” Chemometrics and Intelligent Laboratory Systems 190: 10–21. https://doi.org/https://doi.org/10.1016/j.chemolab.2019.05.004. Ripley, B D. 1987. Stochastic Simulation. Vol. 2009. Mc. John Wiley &amp; Sons. https://doi.org/10.1002/9780470316726. Sæbø, Solve, Trygve Almøy, Arnar Flatberg, Are H. Aastveit, and Harald Martens. 2008. “LPLS-regression: a method for prediction and classification under the influence of background information on predictor variables.” Chemometrics and Intelligent Laboratory Systems 91 (2): 121–32. https://doi.org/10.1016/j.chemolab.2007.10.006. Sæbø, Solve, Trygve Almøy, and Inge S. Helland. 2015. “Simrel - A versatile tool for linear model data simulation based on the concept of a relevant subspace and relevant predictors.” Chemometrics and Intelligent Laboratory Systems 146: 128–35. https://doi.org/10.1016/j.chemolab.2015.05.012. Wentzell, Peter D., and Lorenzo Vega Montoto. 2003. “Comparison of Principal Components Regression and Partial Least Squares Regression Through Generic Simulations of Complex Mixtures.” Chemometrics and Intelligent Laboratory Systems 65 (2): 257–79. https://doi.org/https://doi.org/10.1016/S0169-7439(02)00138-7. "]
]
